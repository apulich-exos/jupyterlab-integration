{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"/Users/bernhard.walter@databricks.com/experiments/test-from-jl\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with mlflow.start_run(run_name=\"test_run\"):\n",
    "    mlflow.log_param(\"x\", 2)\n",
    "    time.sleep(2)\n",
    "    mlflow.log_param(\"a\", 2)\n",
    "    mlflow.log_param(\"s\", \"value\")\n",
    "    mlflow.log_metric(\"mse\", 2.34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = dbbrowser.experiments(experiment_name)\n",
    "runs = mb.get_runs()\n",
    "mb.display(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"/Users/bernhard.walter@databricks.com/experiments/keras-from-jl\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The following import and function call are the only additions to code required\n",
    "# to automatically log metrics and parameters to MLflow.\n",
    "import mlflow.keras\n",
    "\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = dbbrowser.experiments(experiment_name)\n",
    "runs = mb.get_runs()\n",
    "mb.display(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib with auto logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"/Users/bernhard.walter@databricks.com/experiments/mllib-from-jl\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.format(\"libsvm\").load(\"/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\")\n",
    "test = spark.read.format(\"libsvm\").load(\"/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "print(\"There are {} training images and {} test images.\".format(training.count(), test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ML classification, indexer, and pipeline classes \n",
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer: Read input column \"label\" (digits) and annotate them as categorical values.\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "# DecisionTreeClassifier: Learn to predict column \"indexedLabel\" using the \"features\" column.\n",
    "dtc = DecisionTreeClassifier(labelCol=\"indexedLabel\")\n",
    "# Chain indexer + dtc together into a single ML Pipeline.\n",
    "pipeline = Pipeline(stages=[indexer, dtc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluation metric.  In this case, use \"weightedPrecision\", which is equivalent to 0-1 accuracy.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"weightedPrecision\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder() \\\n",
    "  .addGrid(dtc.maxDepth, [2, 4, 6, 8]) \\\n",
    "  .addGrid(dtc.maxBins, [2, 4, 8]) \\\n",
    "  .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=grid, numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.mleap\n",
    "with mlflow.start_run():\n",
    "  cvModel = cv.fit(training)\n",
    "  mlflow.set_tag('owner_team', 'UX Data Science') # Logs user-defined tags\n",
    "  test_metric = evaluator.evaluate(cvModel.transform(test))\n",
    "  mlflow.log_metric('test_' + evaluator.getMetricName(), test_metric) # Logs additional metrics\n",
    "  mlflow.mleap.log_model(spark_model=cvModel.bestModel, sample_input=test, artifact_path='best-model') # Logs the best model via mleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = dbbrowser.experiments(experiment_name)\n",
    "runs = mb.get_runs()\n",
    "mb.display(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_runs = runs[runs.parent_run_id.str.startswith(\"bfd09a3f\") & (runs[\"M.avg_weightedPrecision\"] > 0.5)]\n",
    "best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.compare(best_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH 0508-164224-fores138 aws-ffm:TEST-6.5.x (dbjl/Spark)",
   "language": "python",
   "name": "ssh__ssh0508-164224-fores138aws-ffmtest-6.5.xdbjlspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
